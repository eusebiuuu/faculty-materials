{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3acc7d14",
      "metadata": {
        "id": "3acc7d14"
      },
      "source": [
        "# Retele de perceptroni - Pytorch & Scikit Learn"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e013ba9",
      "metadata": {
        "id": "4e013ba9"
      },
      "source": [
        "### Definirea unei retele de perceptroni in Scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d7e36b6e",
      "metadata": {
        "id": "d7e36b6e"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "mlp_classifier_model = MLPClassifier(hidden_layer_sizes=(100, ),\n",
        "                                     activation='relu',\n",
        "                                     solver='adam',\n",
        "                                     alpha=0.0001,\n",
        "                                     batch_size='auto',\n",
        "                                     learning_rate='constant',\n",
        "                                     learning_rate_init=0.001,\n",
        "                                     power_t=0.5,\n",
        "                                     max_iter=200,\n",
        "                                     shuffle=True,\n",
        "                                     random_state=None,\n",
        "                                     tol=0.0001,\n",
        "                                     momentum=0.9,\n",
        "                                     early_stopping=False,\n",
        "                                     validation_fraction=0.1,\n",
        "                                     n_iter_no_change=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b399b9c5",
      "metadata": {
        "id": "b399b9c5"
      },
      "source": [
        "Parametri:\n",
        "- *hidden_layer_sizes* (tuple, default=(100, )): Un tuplu cu \"n\" elemente; al i-lea element reprezinta numarul de neurori din al i-lea strat ascuns.\n",
        "\n",
        "(*hidden_layer_sizes=(neuroni_strat1, neuroni_strat2, neuroni_strat3, ...)* - Default: O retea cu un strat ascuns cu 100 de neuroni\n",
        "\n",
        "- *activation*( {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default=‘relu’)\n",
        "  - ‘Identity’: 𝑓(𝑥) = 𝑥\n",
        "  - ‘logistic’ : 𝑓(𝑥) = (1 + e^(−𝑥))^(-1)\n",
        "  - ‘tanh’ : 𝑓(𝑥) = 𝑡𝑎𝑛ℎ(𝑥)\n",
        "  - ‘relu’ : 𝑓(𝑥) = 𝑚𝑎𝑥(0, 𝑥)\n",
        "  \n",
        "- *solver* ( {‘lbfgs’, ‘sgd’, ‘adam’}, default=‘adam’): regula de invatare (update)\n",
        "  - ‘sgd’ - stochastic gradient descent (doar pe acesta il vom folosi).\n",
        "\n",
        "- *alpha* (float, default=0.0001): parametru pentru regularizare L2.\n",
        "\n",
        "- *batch_size*: (int, default=‘auto’)\n",
        "  - auto - marimea batch-ului pentru antrenare este min(200, n_samples).\n",
        "\n",
        "- *learning_rate* ( {‘constant’, ‘invscaling’, ‘adaptive’}, default=‘constant’ ):\n",
        "  - ‘constant’ : rata de invatare este constanta si este data de parametrul\n",
        "  learning_rate_init.\n",
        "  - ‘invscaling’: rata de invatare va fi scazuta la fiecare pas t, dupa\n",
        "  formula: new_learning_rate = learning_rate_init / pow(t, power_t)\n",
        "  - ‘adaptive’: pastreaza rata de invatare constanta cat timp eroarea\n",
        "  scade. Daca eroarea nu scade cu cel putin tol (fata de epoca anterior)\n",
        "  sau daca scorul pe multimea de validare (doar daca\n",
        "  ealy_stopping=True) nu creste cu cel putin tol (fata de epoca\n",
        "  anteriora), rata de invatare curenta se imparte la 5.\n",
        "\n",
        "- *learning_rate_init* (double, default=0.001): rata de invatare\n",
        "- *power_t* (double, default=0.5): parametrul pentru learning_rate=’invscaling’.\n",
        "- *max_iter* (int, default=200): numarul maxim de epoci pentru antrenare.\n",
        "- *shuffle* (bool, default=True): amesteca datele la fiecare epoca\n",
        "- *tol* (float, default=1e-4) :\n",
        "  - Daca eroarea sau scorul nu se imbunatatesc timp n_iter_no_chage\n",
        "epoci consecutive (si learning_rate != ‘adaptive’) cu cel putin tol,\n",
        "antrenarea se opreste.\n",
        "- *momentum* (float, default=0.9): - valoarea pentru momentum cand se\n",
        "foloseste gradient descent cu momentum. Trebuie sa fie intre 0 si 1.\n",
        "- *early_stopping* (bool, default=False):\n",
        "  - Daca este setat cu True atunci antrenarea se va termina daca eroarea\n",
        "pe multimea de validare nu se imbunatateste timp n_iter_no_chage\n",
        "epoci consecutive cu cel putin tol.\n",
        "- *validation_fraction* (float, optional, default=0.1):\n",
        "  - Procentul din multimea de antrenare care sa fie folosit pentru validare\n",
        "(doar cand early_stopping=True). Trebuie sa fie intre 0 si 1.\n",
        "- *n_iter_no_change* : (int, optional, default 10, sklearn-versiune-0.20)\n",
        "  - Numarul maxim de epoci fara imbunatatiri (eroare sau scor)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3314f407",
      "metadata": {
        "id": "3314f407"
      },
      "source": [
        "Mai departe in restul laboratorului ne vom focusa pe implementara retelelor neuronale folosind libraria Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a389f77",
      "metadata": {
        "id": "5a389f77"
      },
      "source": [
        "### Install Pytorch\n",
        "\n",
        "\n",
        "Accesati linkul: https://pytorch.org, iar la sectiunea \"Install Pytorch\" selectati detaliile conform specificatiilor masinii voastre. Mai precis, daca masina dispune de o placa video atunci lasati selectia nemodificata, in caz contrar selectati CPU in campul \"Compute Platform\".\n",
        "\n",
        "\n",
        "Pentru a verifica daca instalarea a fost cu succes, puteti rula urmatorul bloc de cod:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a886e425",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a886e425",
        "outputId": "81a87e4d-ed7b-4257-b03a-921e2de70004"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0262, 0.8300, 0.1864],\n",
            "        [0.0230, 0.8618, 0.2052],\n",
            "        [0.8117, 0.2919, 0.4342],\n",
            "        [0.9858, 0.7974, 0.7526],\n",
            "        [0.0235, 0.2041, 0.4412]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.rand(5, 3)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "826a10b1",
      "metadata": {
        "id": "826a10b1"
      },
      "source": [
        "Pentru a verifica daca GPU-ul este accesibil de catre Pytorch, puteti rula codul urmator. Daca totul este in regula, ultima linie ar trebui sa returneze True."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "644ed1a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "644ed1a4",
        "outputId": "dc9acf6e-2a23-4903-f93e-c290bdec5d66"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ME-mtkmEKICE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME-mtkmEKICE",
        "outputId": "be04b42e-740f-46e0-bfc0-99e305449eb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fri Apr 11 00:37:05 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8              9W /   70W |       2MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119c79ea",
      "metadata": {
        "id": "119c79ea"
      },
      "source": [
        "### Definirea retelei neuronale"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e098ff23",
      "metadata": {
        "id": "e098ff23"
      },
      "source": [
        "Pentru a crea un model in Pytorch este necesar sa extindem clasa **nn.Module**, iar in constructor vom defini straturile retelei care vor fi folosite in implementarea functiei **forward**. Mai jos aveti un exemplu pentru un Multilayer Perceptron cu un singur strat ascuns.\n",
        "\n",
        "- stratul **Flatten** transforma datele de intrare in vectori 1-dimensionali.\n",
        "- stratul **Linear** aplica o transformare liniara: xW<sup>T</sup>+b. Pentru acest strat trebuie sa specificam dimensiunile matricei W, care corespund cu dimensiunea tensorilor de intrare si iesire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3fb817ba",
      "metadata": {
        "id": "3fb817ba"
      },
      "outputs": [],
      "source": [
        "# Varianta 1, mai explicita, lunga, si flexibila la logica adaugata intre straturi (control)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.first_layer = nn.Linear(28 * 28, 512) # FC1\n",
        "        # self.dropout1 = nn.Dropout(p=0.2)\n",
        "        self.second_layer = nn.Linear(512, 512) # FC2\n",
        "        # self.dropout2 = nn.Dropout(p=0.2)\n",
        "        self.output_layer = nn.Linear(512, 10) # FC3\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.first_layer(x))\n",
        "        # x = self.dropout1(x)\n",
        "        x = F.relu(self.second_layer(x))\n",
        "        # x = self.dropout2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Xadb0FipCW9t",
      "metadata": {
        "id": "Xadb0FipCW9t"
      },
      "outputs": [],
      "source": [
        "# Varianta 2, mai compacta si eleganta pentru retele simple (dar mai rigida la schimbari)\n",
        "import torch.nn as nn\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(28 * 28, 512),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(p=0.2),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            # nn.Dropout(p=0.2),\n",
        "            nn.Linear(512, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "model = NeuralNetwork().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XNXQCwQHD7j_",
      "metadata": {
        "id": "XNXQCwQHD7j_"
      },
      "source": [
        "Putem sa si fortam device-ul pe care antrenam, cu:\n",
        "\n",
        "\n",
        "```\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "```\n",
        "\n",
        "sau\n",
        "\n",
        "```\n",
        "device = torch.device(\"cpu\") / device = torch.device(\"cuda\") / device = torch.device(\"cuda:1\") - daca avem mai multe placi video\n",
        "```\n",
        "\n",
        "dupa care ii dam:\n",
        "\n",
        "```\n",
        "model.to(device)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "860ee12d",
      "metadata": {
        "id": "860ee12d"
      },
      "source": [
        "Trecerea unui exemplu prin reteaua precedenta se poate executa in felul urmator:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12088a6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12088a6a",
        "outputId": "8abe2713-5234-4909-bcf3-d2c9df61544f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[-0.0025, -0.0220,  0.0092,  0.0460,  0.0248, -0.1026,  0.0115, -0.0041,\n",
              "          0.0397,  0.0023],\n",
              "        [ 0.0270, -0.0269,  0.0483,  0.0569, -0.0073, -0.1117,  0.1061,  0.0082,\n",
              "          0.0087,  0.0214],\n",
              "        [ 0.0053, -0.0357,  0.0217,  0.0450, -0.0064, -0.1106,  0.0631,  0.0425,\n",
              "          0.0024,  0.0560],\n",
              "        [-0.0077, -0.0647,  0.0059,  0.0206,  0.0425, -0.1246,  0.0336,  0.0064,\n",
              "          0.0097,  0.0556],\n",
              "        [ 0.0439, -0.0590, -0.0068, -0.0175,  0.0175, -0.1118,  0.0483,  0.0035,\n",
              "          0.0362,  0.0694]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = NeuralNetwork()\n",
        "model(torch.rand(5, 1, 28, 28))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9cffd860",
      "metadata": {
        "id": "9cffd860"
      },
      "source": [
        "### Antrenarea retelei"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d74a382",
      "metadata": {
        "id": "2d74a382"
      },
      "source": [
        "Pentru antrenarea retelei avem nevoie de date de antrenare, un algoritm de optimizare si o functie de pierdere pe care sa o minimizam pe setul de antrenare.\n",
        "\n",
        "Vom folosi MNIST pentru a ilustra o procedura de antrenare in Pytorch, ca algoritm de optimizare vom folosi stochastic gradient descent (SGD), iar functia de optimizare va fi cross entropy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5a44d27",
      "metadata": {
        "id": "e5a44d27"
      },
      "source": [
        "Crearea seturilor de date si a dataloader-lor care ne vor ajuta sa iteram prin batch-uri in timpul unei epoci:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7154e5bf",
      "metadata": {
        "id": "7154e5bf"
      },
      "outputs": [],
      "source": [
        "# Varianta 1, mai explicita\n",
        "\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FYAHHaIzHJqG",
      "metadata": {
        "id": "FYAHHaIzHJqG"
      },
      "outputs": [],
      "source": [
        "# Varianta 2, mai eleganta si mai robusta la schimbari/transformari\n",
        "\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToTensor()                                     # -> Transforma si normalizeaza imaginea in tensori in intervalul 0-1\n",
        "#     transforms.Normalize((0.1307,), (0.3081,))                # -> Normalizeaza datele cu (mean, std)\n",
        "#     transforms.RandomRotation(15)                             # -> Roteste imaginea aleatoriu cu un unghi intre -15 si 15\n",
        "#     transforms.RandomHorizontalFlip()                         # -> Intoarce imaginea orizontal cu probabilitate implicita de 0.5\n",
        "#     transforms.RandomVerticalFlip()                           # -> La fel ^, doar ca intoarce imaginea vertical\n",
        "#     transforms.RandomCrop(24)                                 # -> Taie aleator o portiune de dimensiunea 24x24 (size x size)\n",
        "#     transforms.Resize((28, 28))                               # -> Face resize la dimensiunile la 28x28\n",
        "#     transforms.ColorJitter(brightness=0.2, contrast=0.2)      # -> Modifica aleator luminozitatea, contrastul, stauratia, etc.\n",
        "#     transforms.Grayscale(num_output_channels=1)               # -> Converteste imaginea de la RGB la Grayscale\n",
        "#     transforms.CenterCrop(24)                                 # -> Taie zona centrala a imaginii\n",
        "# ])\n",
        "\n",
        "transform = transforms.ToTensor()\n",
        "\n",
        "train_data = datasets.MNIST(root=\"data\", train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root=\"data\", train=False, download=True, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FE9FxB1fb8Ey",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE9FxB1fb8Ey",
        "outputId": "e615cb85-75e5-401d-e037-f55dd3172cd7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'dataset': Dataset MNIST\n",
              "     Number of datapoints: 60000\n",
              "     Root location: data\n",
              "     Split: Train\n",
              "     StandardTransform\n",
              " Transform: ToTensor(),\n",
              " 'num_workers': 0,\n",
              " 'prefetch_factor': None,\n",
              " 'pin_memory': False,\n",
              " 'pin_memory_device': '',\n",
              " 'timeout': 0,\n",
              " 'worker_init_fn': None,\n",
              " '_DataLoader__multiprocessing_context': None,\n",
              " 'in_order': True,\n",
              " '_dataset_kind': 0,\n",
              " 'batch_size': 64,\n",
              " 'drop_last': False,\n",
              " 'sampler': <torch.utils.data.sampler.SequentialSampler at 0x7d1fff297390>,\n",
              " 'batch_sampler': <torch.utils.data.sampler.BatchSampler at 0x7d1fff424e90>,\n",
              " 'generator': None,\n",
              " 'collate_fn': <function torch.utils.data._utils.collate.default_collate(batch)>,\n",
              " 'persistent_workers': False,\n",
              " '_DataLoader__initialized': True,\n",
              " '_IterableDataset_len_called': None,\n",
              " '_iterator': None}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataloader.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33dfd515",
      "metadata": {
        "id": "33dfd515"
      },
      "source": [
        "Crearea modelului si definirea algoritmului de optimizare:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "uueDq3YNLlbW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uueDq3YNLlbW",
        "outputId": "3ac0f170-f03c-4b81-a313-c86dc9ffa52d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "Loss: 2.313639  [    0/60000]\n",
            "Loss: 0.289800  [ 6400/60000]\n",
            "Loss: 0.183688  [12800/60000]\n",
            "Loss: 0.245247  [19200/60000]\n",
            "Loss: 0.107182  [25600/60000]\n",
            "Loss: 0.333738  [32000/60000]\n",
            "Loss: 0.141004  [38400/60000]\n",
            "Loss: 0.240234  [44800/60000]\n",
            "Loss: 0.304911  [51200/60000]\n",
            "Loss: 0.173267  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 95.6%, Avg loss: 0.138787 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "Loss: 0.093351  [    0/60000]\n",
            "Loss: 0.109368  [ 6400/60000]\n",
            "Loss: 0.099886  [12800/60000]\n",
            "Loss: 0.098969  [19200/60000]\n",
            "Loss: 0.037061  [25600/60000]\n",
            "Loss: 0.141523  [32000/60000]\n",
            "Loss: 0.072462  [38400/60000]\n",
            "Loss: 0.125026  [44800/60000]\n",
            "Loss: 0.154892  [51200/60000]\n",
            "Loss: 0.106008  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.7%, Avg loss: 0.109013 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "Loss: 0.039883  [    0/60000]\n",
            "Loss: 0.036797  [ 6400/60000]\n",
            "Loss: 0.043843  [12800/60000]\n",
            "Loss: 0.061818  [19200/60000]\n",
            "Loss: 0.044012  [25600/60000]\n",
            "Loss: 0.068877  [32000/60000]\n",
            "Loss: 0.064585  [38400/60000]\n",
            "Loss: 0.094429  [44800/60000]\n",
            "Loss: 0.081539  [51200/60000]\n",
            "Loss: 0.087873  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.4%, Avg loss: 0.092964 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "Loss: 0.019834  [    0/60000]\n",
            "Loss: 0.011801  [ 6400/60000]\n",
            "Loss: 0.021188  [12800/60000]\n",
            "Loss: 0.135366  [19200/60000]\n",
            "Loss: 0.019898  [25600/60000]\n",
            "Loss: 0.033202  [32000/60000]\n",
            "Loss: 0.042469  [38400/60000]\n",
            "Loss: 0.049136  [44800/60000]\n",
            "Loss: 0.057992  [51200/60000]\n",
            "Loss: 0.041724  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.6%, Avg loss: 0.089082 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "Loss: 0.030422  [    0/60000]\n",
            "Loss: 0.007329  [ 6400/60000]\n",
            "Loss: 0.008894  [12800/60000]\n",
            "Loss: 0.034534  [19200/60000]\n",
            "Loss: 0.021156  [25600/60000]\n",
            "Loss: 0.063404  [32000/60000]\n",
            "Loss: 0.016411  [38400/60000]\n",
            "Loss: 0.044374  [44800/60000]\n",
            "Loss: 0.010072  [51200/60000]\n",
            "Loss: 0.050648  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 96.0%, Avg loss: 0.175793 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "Loss: 0.004862  [    0/60000]\n",
            "Loss: 0.086313  [ 6400/60000]\n",
            "Loss: 0.018233  [12800/60000]\n",
            "Loss: 0.010666  [19200/60000]\n",
            "Loss: 0.006674  [25600/60000]\n",
            "Loss: 0.005942  [32000/60000]\n",
            "Loss: 0.013710  [38400/60000]\n",
            "Loss: 0.045510  [44800/60000]\n",
            "Loss: 0.024363  [51200/60000]\n",
            "Loss: 0.009639  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.9%, Avg loss: 0.081921 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "Loss: 0.008396  [    0/60000]\n",
            "Loss: 0.025595  [ 6400/60000]\n",
            "Loss: 0.031485  [12800/60000]\n",
            "Loss: 0.021101  [19200/60000]\n",
            "Loss: 0.004799  [25600/60000]\n",
            "Loss: 0.012661  [32000/60000]\n",
            "Loss: 0.003242  [38400/60000]\n",
            "Loss: 0.012916  [44800/60000]\n",
            "Loss: 0.019269  [51200/60000]\n",
            "Loss: 0.008524  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.101516 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "Loss: 0.037470  [    0/60000]\n",
            "Loss: 0.001511  [ 6400/60000]\n",
            "Loss: 0.022429  [12800/60000]\n",
            "Loss: 0.001472  [19200/60000]\n",
            "Loss: 0.073851  [25600/60000]\n",
            "Loss: 0.085300  [32000/60000]\n",
            "Loss: 0.021037  [38400/60000]\n",
            "Loss: 0.007072  [44800/60000]\n",
            "Loss: 0.069610  [51200/60000]\n",
            "Loss: 0.058962  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.100638 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "Loss: 0.011903  [    0/60000]\n",
            "Loss: 0.000919  [ 6400/60000]\n",
            "Loss: 0.023413  [12800/60000]\n",
            "Loss: 0.006779  [19200/60000]\n",
            "Loss: 0.003012  [25600/60000]\n",
            "Loss: 0.010394  [32000/60000]\n",
            "Loss: 0.044600  [38400/60000]\n",
            "Loss: 0.003774  [44800/60000]\n",
            "Loss: 0.097398  [51200/60000]\n",
            "Loss: 0.042811  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 97.7%, Avg loss: 0.111725 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "Loss: 0.031144  [    0/60000]\n",
            "Loss: 0.000304  [ 6400/60000]\n",
            "Loss: 0.052685  [12800/60000]\n",
            "Loss: 0.014752  [19200/60000]\n",
            "Loss: 0.007087  [25600/60000]\n",
            "Loss: 0.023063  [32000/60000]\n",
            "Loss: 0.000904  [38400/60000]\n",
            "Loss: 0.013153  [44800/60000]\n",
            "Loss: 0.155034  [51200/60000]\n",
            "Loss: 0.002963  [57600/60000]\n",
            "Test Error: \n",
            " Accuracy: 98.0%, Avg loss: 0.095348 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "model = NeuralNetwork()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# Functia de antrenare a retelei\n",
        "\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f\"Loss: {loss.item():>7f}  [{batch * len(X):>5d}/{len(dataloader.dataset):>5d}]\")\n",
        "\n",
        "# Metoda de testare a performantei retelei:\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= len(dataloader)\n",
        "    correct /= len(dataloader.dataset)\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for t in range(num_epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82bffe37",
      "metadata": {
        "id": "82bffe37"
      },
      "source": [
        "### Exercitii"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16d413e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.first_layer = nn.Linear(28 * 28, 512) # FC1\n",
        "        # self.dropout1 = nn.Dropout(p=0.2)\n",
        "        self.second_layer = nn.Linear(512, 512) # FC2\n",
        "        # self.dropout2 = nn.Dropout(p=0.2)\n",
        "        self.output_layer = nn.Linear(512, 10) # FC3\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.first_layer(x))\n",
        "        # x = self.dropout1(x)\n",
        "        x = F.relu(self.second_layer(x))\n",
        "        # x = self.dropout2(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "615c3cb2",
      "metadata": {
        "id": "615c3cb2"
      },
      "source": [
        "1. Antrenati o retea de perceptroni care sa clasifice cifrele scrise de mana MNIST. Datele trebuie normalizate prin scaderea mediei si impartirea la deviatia standard. Antrenati pentru 5 epoci si testati urmatoarele configuratii de retele:\n",
        "\n",
        "a. Definiti o retea cu un singur strat ascuns cu un singur neuron si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-2.\n",
        "\n",
        "b. Definiti o retea cu un singur strat ascuns cu 10 neuroni si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-2.\n",
        "\n",
        "c. Definiti o retea cu un singur strat ascuns cu 10 neuroni si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-5.\n",
        "\n",
        "d. Definiti o retea cu un singur strat ascuns cu 10 neuroni si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 10.\n",
        "\n",
        "e. Definiti o retea cu 2 straturi ascunse cu 10 neuroni fiecare si folositi ca functie de activare tanh. Pentru optimizator folositi un learning rate de 1e-2.\n",
        "\n",
        "f. Definiti o retea cu 2 straturi ascunse cu 10 neuroni fiecare si folositi ca functie de activare relu. Pentru optimizator folositi un learning rate de 1e-2.\n",
        "\n",
        "g. Definiti o retea cu 2 straturi ascunse cu 100 neuroni fiecare si folositi ca functie de activare relu. Pentru optimizator folositi un learning rate de 1e-2.\n",
        "\n",
        "h. Definiti o retea cu 2 straturi ascunse cu 100 neuroni fiecare si folositi ca functie de activare relu. Pentru optimizator folositi un learning rate de 1e-2 si momentum=0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t8c-VuCWamTg",
      "metadata": {
        "id": "t8c-VuCWamTg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9502857142857143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/eusebiuu/anaconda3/envs/ml_practice/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (5) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "from torch import tensor, float32, int16\n",
        "\n",
        "X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
        "\n",
        "# Preprocessing\n",
        "y = y.astype(int)\n",
        "X_tensor = tensor(X, dtype=float32)\n",
        "y_tensor = tensor(y, dtype=int16)\n",
        "\n",
        "# Splitting\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n",
        "\n",
        "# Normalization\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(x_train)\n",
        "\n",
        "# print(scaler.mean_) # media\n",
        "# print(scaler.scale_) # deviatia standard\n",
        "\n",
        "scaled_x_train = scaler.transform(x_train)\n",
        "\n",
        "scaled_x_test = scaler.transform(x_test)\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "mlp_classifier_model = MLPClassifier(hidden_layer_sizes=(100, 100, ),\n",
        "                                     activation='relu',\n",
        "                                     solver='adam',\n",
        "                                     alpha=0.0001,\n",
        "                                     batch_size='auto',\n",
        "                                     learning_rate='constant',\n",
        "                                     learning_rate_init=1e-2,\n",
        "                                     power_t=0.5,\n",
        "                                     max_iter=EPOCHS,\n",
        "                                     shuffle=True,\n",
        "                                     random_state=None,\n",
        "                                     tol=0.0001,\n",
        "                                     momentum=0.9,\n",
        "                                     early_stopping=False,\n",
        "                                     validation_fraction=0.1,\n",
        "                                     n_iter_no_change=10)\n",
        "\n",
        "mlp_classifier_model.fit(scaled_x_train, y_train)\n",
        "\n",
        "predicted_labels = mlp_classifier_model.predict(scaled_x_test)\n",
        "\n",
        "print(accuracy_score(y_test, predicted_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ml_practice",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
