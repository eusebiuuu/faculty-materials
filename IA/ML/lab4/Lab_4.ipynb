{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLsj94nwDag0"
   },
   "source": [
    "# Laborator 4 - Data Normalization and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper methods & examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V3r5E2Q8F7gk",
    "outputId": "21f29c90-b7ee-4e26-b6cd-1185a2eecec5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.         0.33333333]\n",
      "[0.81649658 0.81649658 1.24721913]\n",
      "[[ 0.         -1.22474487  1.33630621]\n",
      " [ 1.22474487  0.         -0.26726124]\n",
      " [-1.22474487  1.22474487 -1.06904497]]\n",
      "[[-2.44948974  1.22474487 -0.26726124]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "x_train = np.array([[1, -1, 2], [2, 0, 0], [0, 1, -1]], dtype=np.float64)\n",
    "x_test = np.array([[-1, 1, 0]], dtype=np.float64)\n",
    "\n",
    "# facem statisticile pe datele de antrenare\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "# afisam media\n",
    "print(scaler.mean_) # => [1. 0. 0.33333333]\n",
    "# afisam deviatia standard\n",
    "print(scaler.scale_) # => [0.81649658 0.81649658 1.24721913]\n",
    "\n",
    "# scalam datele de antrenare\n",
    "scaled_x_train = scaler.transform(x_train)\n",
    "print(scaled_x_train) # => [[0. -1.22474487 1.33630621]\n",
    "# [1.22474487 0. -0.26726124]\n",
    "# [-1.22474487 1.22474487 -1.06904497]]\n",
    "\n",
    "# scalam datele de test\n",
    "scaled_x_test = scaler.transform(x_test)\n",
    "print(scaled_x_test) # => [[-2.44948974 1.22474487 -0.26726124]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercitii"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Descarcati si cititi datele de train si de test (atentie la formatul mesajelor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "training_sentences = np.load(\"./data/training_sentences.npy\", allow_pickle=True)\n",
    "test_sentences = np.load(\"./data/test_sentences.npy\", allow_pickle=True)\n",
    "train_labels = np.load(\"./data/training_labels.npy\", allow_pickle=True)\n",
    "test_labels = np.load(\"./data/test_labels.npy\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5d-tf9ZUVeK"
   },
   "source": [
    "2. Definiți funcția normalize_data(train_data, test_data, type=None) care primește ca\n",
    "parametri datele de antrenare, respectiv de testare și tipul de normalizare ({None,\n",
    "‘standard’, ‘l1’, ‘l2’}) și întoarce aceste date normalizate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "tMBbRDGMDPP0"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "import numpy as np\n",
    "\n",
    "def normalize_data(train_data, test_data, type=None):\n",
    "    if type is None:\n",
    "        return (train_data, test_data)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(train_data)\n",
    "    \n",
    "    standard_train = scaler.transform(train_data)\n",
    "    standard_test = scaler.transform(test_data)\n",
    "    \n",
    "    if type == \"l1\" or type == \"l2\":\n",
    "        train_array = np.array(standard_train)\n",
    "        test_array = np.array(standard_test)\n",
    "        \n",
    "        transformer = Normalizer(type)\n",
    "        normalized_train = transformer.transform(train_array)\n",
    "        normalized_test = transformer.transform(test_array)\n",
    "        \n",
    "        return (normalized_train, normalized_test)\n",
    "    \n",
    "    return (standard_train, standard_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wzydHSuMDVWE"
   },
   "source": [
    "3. Definiți clasa BagOfWords în al cărui constructor se inițializează vocabularul (un\n",
    "dicționar gol). În cadrul ei implementați metoda build_vocabulary(self, data) care\n",
    "primește ca parametru o listă de mesaje(listă de liste de strings) și construiește\n",
    "vocabularul pe baza acesteia. Cheile dicționarului sunt reprezentate de cuvintele din\n",
    "eseuri, iar valorile de id-urile unice atribuite cuvintelor. Pe lângă vocabularul pe care-l\n",
    "construiți, rețineți și o listă cu cuvintele în ordinea adăugării în vocabular.\n",
    "Afișați dimensiunea vocabularul construit (9522).\n",
    "OBS. Vocabularul va fi construit doar pe baza datelor din setul de antrenare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Definiți metoda get_features(self, data) care primește ca parametru o listă de\n",
    "mesaje de dimensiune num_samples(listă de liste de strings) și returnează o matrice\n",
    "de dimensiune (num_samples x dictionary_length) definită astfel:\n",
    "features(sample_idx, word_idx) = numarul de aparitii al\n",
    "cuvantului cu id − ul word_idx in documentul sample_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.vocabulary = {}             # Dictionar {word: id}\n",
    "        self.word_list = []              # Lista in ordinea adaugarii\n",
    "\n",
    "    def build_vocabulary(self, data):\n",
    "        unique_id = 1\n",
    "        for message in data:\n",
    "            for word in message:\n",
    "                if self.vocabulary.get(word, None) is None:\n",
    "                    self.vocabulary[word] = unique_id\n",
    "                    unique_id += 1\n",
    "                    self.word_list.append(word)\n",
    "        print(len(self.vocabulary))\n",
    "    \n",
    "    def get_features(self, data):\n",
    "        features = np.zeros((len(data), len(self.word_list)))\n",
    "        for i, message in enumerate(data):\n",
    "            for j, word in enumerate(self.word_list):\n",
    "                features[i][j] = message.count(word)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HFH9TPMRDS1L"
   },
   "source": [
    "5. Cu ajutorul funcțiilor definite anterior, obțineți reprezentările BOW pentru mulțimea de\n",
    "antrenare și testare, apoi normalizați-le folosind norma “L2”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "TSk4Dp8DDX7k"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9522\n"
     ]
    }
   ],
   "source": [
    "BOW = BagOfWords()\n",
    "BOW.build_vocabulary(training_sentences)\n",
    "\n",
    "train_features = BOW.get_features(training_sentences)\n",
    "test_features = BOW.get_features(test_sentences)\n",
    "train_data, test_data = normalize_data(train_features, test_features, type = \"l2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BvN2ZAeDUOM"
   },
   "source": [
    "6. a) Antrenați un SVM cu kernel linear care să clasifice mesaje în mesaje\n",
    "spam/non-spam. Pentru parametrul C setați valoarea 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 1\n",
      " 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svc_model = LinearSVC()\n",
    "svc_model.fit(train_features, train_labels)\n",
    "\n",
    "estimated_labels = svc_model.predict(test_features)\n",
    "\n",
    "print(estimated_labels[1:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "41774_U4Xv9Z"
   },
   "source": [
    "b) Calculați acuratețea și\n",
    "F1-score pentru mulțimea de testare.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9853260869565217\n",
      "F1-score: 0.9443298969072165\n"
     ]
    }
   ],
   "source": [
    "print(svc_model.score(test_features, test_labels))\n",
    "\n",
    "TP, FP, FN = 0, 0, 0\n",
    "\n",
    "for i, label in enumerate(estimated_labels):\n",
    "    TP += label == 1 and test_labels[i] == 1\n",
    "    FP += label == 1 and test_labels[i] == 0\n",
    "    FN += label == 0 and test_labels[i] == 1\n",
    "\n",
    "print(f\"F1-score: {(2 * TP) / (2 * TP + FP + FN)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw_LmLe_X0_h"
   },
   "source": [
    "c) Afișați cele mai negative (spam) 10 cuvinte și cele mai pozitive (non-spam) 10\n",
    "cuvinte.\n",
    "the first 10 negative words are ['Text' 'To' 'mobile' 'CALL' 'FREE' 'txt' '&' 'Call' 'Txt'\n",
    "'STOP']\n",
    "the first 10 positive words are ['&lt#&gt' 'me' 'i' 'Going' 'him' 'Ok' 'I' 'Ill' 'my' 'Im']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['&lt#&gt', 'him', 'Oh', 'Alright', 'me', 'always', 'right', 'It', 'Ill', 'Waiting']\n",
      "['ringtoneking', '84484', 'REAL', 'won', 'FREE>RingtoneReply', '85233', 'Txt', 'httptms', 'widelivecomindex', 'For']\n"
     ]
    }
   ],
   "source": [
    "coefficients = svc_model.coef_[0]\n",
    "sorted_idx = coefficients.argsort()\n",
    "print([BOW.word_list[idx] for idx in sorted_idx[0:10]])\n",
    "print([BOW.word_list[idx] for idx in sorted_idx[-10:]][::-1])"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml_practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
